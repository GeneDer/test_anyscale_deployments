# anyscale service deploy -f rayllm_serve_config.yaml
applications:
  - args:
      llm_configs:
        - accelerator_type: A10G
          deployment_config:
            autoscaling_config:
              initial_replicas: 10
              min_replicas: 10
              max_replicas: 10
            max_ongoing_requests: 64
          engine_kwargs:
            max_num_batched_tokens: 4096
            max_num_seqs: 64
            tokenizer_pool_extra_config:
              runtime_env:
                pip: null
            tokenizer_pool_size: 2
            trust_remote_code: true
          generation_config:
            prompt_format:
              assistant: "<|start_header_id|>assistant<|end_header_id|>\n\n{instruction}<|eot_id|>"
              bos: <|begin_of_text|>
              default_system_message: ''
              system: "<|start_header_id|>system<|end_header_id|>\n\n{instruction}<|eot_id|>"
              system_in_user: false
              trailing_assistant: "<|start_header_id|>assistant<|end_header_id|>\n\n"
              user: "<|start_header_id|>user<|end_header_id|>\n\n{instruction}<|eot_id|>"
            stopping_sequences: [ ]
            stopping_tokens:
              - 128001
              - 128009
          input_modality: text
          json_mode:
            enabled: false
          llm_engine: VLLMEngine
          lora_config: null
          max_request_context_length: 4096
          model_loading_config:
            model_id: meta-llama/Meta-Llama-3-8B-Instruct
            model_source: meta-llama/Meta-Llama-3-8B-Instruct
          runtime_env:
            env_vars:
              HUGGING_FACE_HUB_TOKEN: REDACTED
          tensor_parallelism:
            degree: 1
    import_path: rayllm:app
    name: llm-endpoint
    route_prefix: /
query_auth_token_enabled: false
compute_config: "endpoints-aica-release-test-aviary-staging"
image_uri: "localhost:5555/anyscale/endpoints_aica:1.0.0-8341"
name: "test_rayllm_direct_ingress"
